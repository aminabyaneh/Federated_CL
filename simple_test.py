# -*- coding: utf-8 -*-
"""walkthrough.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/phlippe/ENCO/blob/main/walkthrough.ipynb

# Efficient Neural Causal Discovery without Acyclicity Constraints

This notebook is intended to give an overview of the functionalities in this repository. We recommend to go through this notebook if you are interested in working with this repository.

## Preparation

This part is only needed when you are running on Google Colab. We clone the repo to have access to the python files. In case you run this notebook locally, this step will be automatically skipped.
"""

import os
import sys

"""For nicer visualizations, we import matplotlib with inline arguments. Additionally, we import PyTorch and Numpy on which the ENCO implementation is based on."""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
from matplotlib_inline.backend_inline import set_matplotlib_formats
set_matplotlib_formats('svg', 'pdf')

import torch
import numpy as np

"""## Causal Graphs

First, we take a look at how we can generate causal graphs and interact with them. All functionalities for this purpose have been summarized in the folder `causal_graphs`, and we import the most important functions below.
"""

from causal_graphs.graph_definition import CausalDAG  # Base class of causal graphs
from causal_graphs.graph_generation import generate_categorical_graph, get_graph_func  # Functions for generating new graphs
from causal_graphs.graph_visualization import visualize_graph  # Plotting the graph in matplotlib

"""Every graph is represented as a `CausalDAG` object that contains a list of variables and an adjacency matrix/edge list to represent the graph structure. The conditional distributions are part of the variable objects. To sample a new data point, we iterate through the variables in the causal order and sample one by one. To demonstrate this, let us first generate an arbitrary graph. This can be done with the `generate_categorical_graph` function, and we can specify the desired graph structure with `get_graph_func`:"""

graph = generate_categorical_graph(num_vars=8,
                                   min_categs=10,
                                   max_categs=10,
                                   graph_func=get_graph_func('random'),
                                   edge_prob=0.4,
                                   seed=42)

"""This function call creates a new graph with 8 variables, each having a distribution over 10 categories, and the graph structure is generated randomly by sampling an edge between any pair of variables with a probability of 0.4. The seed ensures that the graph generation is reproducible. To generate other graph structures, simply replace the string `'random'` by e.g. `'chain'` or `'jungle'`.

To get an intuition of what the graph actually looks like, we can print it:
"""

print(graph)

"""The variables are named alphabetically, and we have 10 edges. The edges are listed below the first line, e.g., we have an edge from D to E, and an edge from D to G. Alternatively, we can also plot the graph with matplotlib: """

# visualize_graph(graph, figsize=(4, 4), show_plot=True)

"""To sample from a graph, we use the function `CausalDAG.sample`:"""

# graph.sample()

"""Sampling with interventions is supported by passing a dictionary with the intended interventions. The interventions can be imperfect, i.e. a new distribution, or perfect, i.e. constant values. We demonstrate here a perfect intervention on the variable C:"""

# graph.sample(interventions={'C': np.array([0])})

"""Graphs can be saved and loaded with the function `save_to_file` and `CausalDAG.load_from_file`. To save the graph as a set of observational and interventional dataset, you can use the function `export_graph` from `graph_export.py`. We used this functionality to export the data to apply other causal discovery methods on. Graphs in the `.bif` format, as from the BnLearn repository, can be loaded via the function `load_graph_file` in `graph_real_world.py`.

## Causal Discovery with ENCO

The graph objects explained above are used to implement the structure learning with ENCO in the folder `causal_discovery`. To run ENCO on such a graph, we simply need to create an `ENCO` object, and run the structure learning via the `discover_graph` function:
"""

from causal_discovery.enco import ENCO

import torch.nn as nn
# prior_info = nn.Parameter(torch.zeroes(num_vars, num_vars))

enco_module = ENCO(graph=graph)
if torch.cuda.is_available():
    print('Found Cuda device!')
    enco_module.to(torch.device('cuda:0'))

predicted_adj_matrix = enco_module.discover_graph(num_epochs=2)

print(f'*** Debug information *** \n Final Gamma: \n {enco_module.gamma} \n Final Theta: \n {enco_module.theta}')

"""After every epoch, the metrics of comparing the current prediction to the ground truth graph are printed out. In the case of the small graph we created above, ENCO finds the graph quite quickly. The return value is the predicted adjacency matrix, and can be passed to a new graph object if you want to visualize the prediction. Hyperparameters for the structure learning process can be passed to the ENCO object in the init-function.

This completes the quick guide through the code. To run experiments on a larger scale, we recommend to use the python files provided in the `experiments` folder. Further, the commands to reproduce the experiments in the paper are provided in `experiments/run_scripts/`.
"""
